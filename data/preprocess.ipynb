{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lo = []\n",
    "path_train = 'vietlao/train2023.lo'\n",
    "\n",
    "with open(path_train, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        train_lo.append(line.strip())\n",
    "f.close()\n",
    "\n",
    "train_vi = []\n",
    "path_train = 'vietlao/train2023.vi'\n",
    "\n",
    "with open(path_train, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        train_vi.append(line.strip())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible_lo = []\n",
    "path_train = 'bible/bible.lo'\n",
    "\n",
    "with open(path_train, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        bible_lo.append(line.strip())\n",
    "f.close()\n",
    "\n",
    "bible_vi = []\n",
    "path_train = 'bible/bible.vi'\n",
    "\n",
    "with open(path_train, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        bible_vi.append(line.strip())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "100000\n",
      "12077\n",
      "12077\n"
     ]
    }
   ],
   "source": [
    "print(len(train_lo))\n",
    "print(len(train_vi))\n",
    "print(len(bible_lo))\n",
    "print(len(bible_vi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lo.extend(bible_lo)\n",
    "train_vi.extend(bible_vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_lo))\n",
    "print(len(train_vi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_lo = []\n",
    "path_dev = 'vietlao/dev2023.lo'\n",
    "\n",
    "with open(path_dev, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        dev_lo.append(line.strip())\n",
    "f.close()\n",
    "\n",
    "dev_vi = []\n",
    "path_dev = 'vietlao/dev2023.vi'\n",
    "\n",
    "with open(path_dev, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        dev_vi.append(line.strip())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(len(dev_lo))\n",
    "print(len(dev_vi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ຖ້າ ເຈົ້າ ຮູ້ສຶກ ຢ້ານ ທີ່ ຈະ ປະກາດ ຂໍ ໃຫ້ ຊ້ອມ ເວົ້າ ໃນ ແບບ ທີ່ ຍັງ ບໍ່ ມີ ເປົ້າ ຫມາຍ ປະກາດ.',\n",
       " 'ຄົນ ຕໍ່າ ຕ້ອຍ ຈໍານວນ ຫຼາຍ ຄົງ ຈະ ຫຼົບ ຫຼີກ ຜູ້ ນໍາ ດັ່ງ ກ່າວ ແທນ ທີ່ ຈະ ຂໍ ຄວາມ ຊ່ວຍເຫຼືອ ຫຼື ການ ຊີ້ ນໍາ ຈາກ ເຂົາ ເຈົ້າ.',\n",
       " '21 ບັດ ນີ້ເມື່ອລາຊິນີ ເຫັນ ຄວາມ ຢ້ານ ກົວ ຂອງ ພວກ ຂ້າ ໃຊ້ ນາງ ກໍ ເລີ່ມ ມີ ຄວາມ ຢ້ານ ກົວ ຫລາຍ ຂຶ້ນ, ຢ້ານ ວ່າ ສິ່ງ ບໍ່ ດີ ຈະ ເກີດ ກັບ ນາງ.',\n",
       " 'ການ ຕັດສິນ ໃຈ ທີ່ ຈະ ປ່ຽນແປງ ກໍ ເປັນ ຂອງ ທ່ານ , ແລະ ເປັນ ຂອງ ທ່ານ ຄົນ ດຽວ.',\n",
       " 'ການ ເລືອກ ທີ່ ພວກ ເຈົ້າ ເຮັດ ໃນ ເວລາ ນີ້ ມີ ຄວາມ ສໍາຄັນ ຕະຫລອດ ການ.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lo[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ຕົວຢ່າງ, ໃນພາກທີ 3, ພວກເຮົາໄດ້ເຫັນວິທີການ Sharad Goel, Winter Mason, ແລະ Duncan Watts (2010) ສ້າງເກມທີ່ເອີ້ນວ່າ Friendsense, ເຊິ່ງຕົວຈິງແລ້ວແມ່ນຮູບພາບທີ່ສວຍງາມໃນການສໍາຫຼວດທັດສະນະຄະຕິ. ໃນບົດທີ 4, ພວກເຮົາໄດ້ເຫັນວິທີທີ່ທ່ານສາມາດສ້າງຂໍ້ມູນຄ່າໃຊ້ຈ່າຍທີ່ບໍ່ຖືກຕ້ອງໂດຍການອອກແບບປະສົບການທີ່ປະຊາຊົນຕ້ອງການຢາກ, ເຊັ່ນ: ການທົດລອງດາວໂຫລດດົນຕີທີ່ຂ້ອຍສ້າງດ້ວຍ Peter Dodds ແລະ Duncan Watts (Salganik, Dodds, and Watts 2006) . ສຸດທ້າຍ, ໃນພາກທີ 5, ພວກເຮົາໄດ້ເຫັນວິທີການ Kevin Schawinski, Chris Lintott ແລະທີມງານ Galaxy Zoo ສ້າງການຮ່ວມມືທີ່ມີຂະຫນາດໃຫຍ່ທີ່ສົ່ງຜົນກະທົບຕໍ່ຫຼາຍກວ່າ 100,000 ຄົນທີ່ຈະເຂົ້າຮ່ວມໃນວຽກງານ labeling ຮູບພາບ (ໃນສອງທິດທາງຂອງຄໍາສັບ) (Lintott et al. 2011) ໃນແຕ່ລະກໍລະນີເຫຼົ່ານີ້, ນັກຄົ້ນຄວ້າໄດ້ສຸມໃສ່ການສ້າງປະສົບການທີ່ດີສໍາລັບຜູ້ເຂົ້າຮ່ວມ, ແລະໃນແຕ່ລະກໍລະນີ, ວິທີການນີ້ເຂົ້າໃຈເຖິງຈຸດປະສົງຂອງຜູ້ເຂົ້າຮ່ວມສາມາດເຮັດໃຫ້ການຄົ້ນຄວ້າໃຫມ່.',\n",
       " 'ການນໍາໃຊ້ lumbar',\n",
       " '★ Ninja Warz ບໍ່\\u200bຈໍາ\\u200bກັດ Hack Tokens & Cheat',\n",
       " \"Dennis DM O'Leary, Ph.D., ໂຮງຮຽນວິທະຍາໄລມະຫາວິທະຍາໄລ Washington\",\n",
       " 'ທອງເຫລືອງ 1mm 2mm 4mm 5mm 5mm 5mm',\n",
       " 'ປະຕູສັງກະສີຈັບ-Z1961',\n",
       " 'ກັ່ນຕອງ MS-5130',\n",
       " 'cushion ບ່ອນນັ່ງ mold 6cm',\n",
       " 'Scalpers ຍິນດີຕ້ອນຮັບ',\n",
       " 'ທີ່ຜ່ານມາ: Pots Nursery NP-3']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lo[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ví dụ, trong chương 3, chúng ta đã thấy Sharad Goel, Winter Mason và Duncan Watts (2010) đã tạo ra một trò chơi có tên là Friendsense thực sự là một khung thông minh xung quanh một cuộc khảo sát thái độ. Trong chương 4, chúng tôi đã thấy cách bạn có thể tạo ra dữ liệu chi phí không thay đổi bằng cách thiết kế thử nghiệm mà mọi người thực sự muốn tham gia, chẳng hạn như thử nghiệm tải xuống nhạc mà tôi đã tạo với Peter Dodds và Duncan Watts (Salganik, Dodds, and Watts 2006) . Cuối cùng, trong chương 5, chúng ta đã thấy Kevin Schawinski, Chris Lintott và nhóm Galaxy Zoo đã tạo ra một sự hợp tác hàng loạt đã thúc đẩy hơn 100.000 người tham gia vào một tác vụ ghi nhãn hình ảnh (trong cả hai giác quan) (Lintott et al. 2011) . Trong mỗi trường hợp này, các nhà nghiên cứu tập trung vào việc tạo ra một trải nghiệm tốt cho những người tham gia, và trong mỗi trường hợp, cách tiếp cận tập trung vào người tham gia này đã kích hoạt các loại nghiên cứu mới.',\n",
       " 'sử dụng mạch',\n",
       " '★ Ninja Warz Unlimited Hack Vàng & Gian lận',\n",
       " \"Dennis DM O'Leary, tiến sĩ, Đại học Y Washington\",\n",
       " 'Đồng 1mm 2 mm 3 mm 4mm 4mm 4mm',\n",
       " 'cửa Kẽm xử lý-Z1961',\n",
       " 'Lọc MS-5130',\n",
       " 'Gối ghế khuôn 6cm',\n",
       " 'Chào mừng bạn',\n",
       " 'Kế tiếp: Vườn ươm Chậu NP-7Q']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vi[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pythainlp import word_tokenize\n",
    "# text = train_lo[-10]\n",
    "# word_tokenize(text, engine=\"newmm\", keep_whitespace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ຖ້າ   ເຈົ້າ   ຮູ້ສຶກ   ຢ້ານ   ທີ່   ຈະ   ປະກາດ   ຂໍ   ໃຫ້   ຊ້ອມ   ເວົ້າ   ໃນ   ແບບ   ທີ່   ຍັງ   ບໍ່   ມີ   ເປົ້າ   ຫມາຍ   ປະກາດ .'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from laonlp import word_tokenize\n",
    "# text = train_lo[-10]\n",
    "text = train_lo[0]\n",
    "list = word_tokenize(text)\n",
    "res = \" \".join(list)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ຖ້າ ເຈົ້າ ຮູ້ສຶກ ຢ້ານ ທີ່ ຈະ ປະກາດ ຂໍ ໃຫ້ ຊ້ອມ ເວົ້າ ໃນ ແບບ ທີ່ ຍັງ ບໍ່ ມີ ເປົ້າ ຫມາຍ ປະກາດ .'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = \" \".join(word for word in list if word.strip())\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize & Detokenize By Library laonlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import laonlp\n",
    "# print(len(laonlp.corpus.lao_dictionary()))\n",
    "# print(len(laonlp.corpus.lao_words()))\n",
    "# print(len(laonlp.corpus.lao_wiktionarydict()))\n",
    "# [word for word in laonlp.corpus.lao_stopwords()]\n",
    "# laonlp.corpus.lao_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laonlp import word_tokenize\n",
    "from pythainlp.tokenize import subword_tokenize\n",
    "import laonlp\n",
    "\n",
    "def tokenizeAndDetokenizeLo(example):\n",
    "    list_of_tokens = word_tokenize(example)\n",
    "    # example = \" \".join(list_of_tokens)\n",
    "    example = \" \".join(word for word in list_of_tokens if word.strip())\n",
    "    # example = \" \".join(word for word in list_of_tokens if (word.strip() and word not in laonlp.corpus.lao_stopwords()))\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chuoi = \"This is a sample    sentence.\"\n",
    "# danh_sach = chuoi.split()  # Nếu không có đối số, mặc định là khoảng trắng\n",
    "# print(danh_sach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hẳn nhiều người thấp kém e sợ và tránh mặt những nhà lãnh đạo ấy , thay vì xin họ giúp đỡ hoặc chỉ dẫn .'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from underthesea import word_tokenize\n",
    "\n",
    "# vi_stopwords = []\n",
    "# path = 'vietnamese-stopwords.txt'\n",
    "\n",
    "# with open(path, encoding='utf-8') as f:\n",
    "#     for line in f:\n",
    "#         vi_stopwords.append(line.strip())\n",
    "# f.close()\n",
    "\n",
    "def tokenizeAndDetokenizeVi(example=train_vi[1]):\n",
    "    list_of_tokens = word_tokenize(example)\n",
    "    example = \" \".join(word for word in list_of_tokens)\n",
    "    # example = \" \".join(word for word in list_of_tokens if word not in vi_stopwords)\n",
    "    return example\n",
    "\n",
    "tokenizeAndDetokenizeVi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ຄ ົ ນ ຕ ໍ ່ າ ຕ ້ ອຍ ຈ ໍ ານວນ ຫ ຼ າຍ ຄ ົ ງ ຈະ ຫ ຼ ົ ບ ຫ ຼ ີ ກ ຜ ູ ້ ນ ໍ າ ດ ັ ່ ງ ກ ່ າວ ແທນ ທ ີ ່ ຈະ ຂ ໍ ຄວາມ ຊ ່ ວຍເຫ ຼ ື ອ ຫ ຼ ື ການ ຊ ີ ້ ນ ໍ າ ຈາກ ເຂ ົ າ ເຈ ົ ້ າ .'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizeAndDetokenizeLo('ຕົວຢ່າງ  ໃນ ພາກ ທີ   ພວກເຮົາ ໄດ້')\n",
    "tokenizeAndDetokenizeLo(train_lo[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "max_len = 128\n",
    "\n",
    "def truncate(data):\n",
    "    if len(data.split(\" \")) > max_len:\n",
    "        # Truncate the string to the maximum length\n",
    "        data = data[:max_len]\n",
    "    return data\n",
    "\n",
    "#Removes URL\n",
    "def remove_url(data):\n",
    "    url = r\"(?i)\\b((?:https?://|www\\.|[a-zA-Z0-9.\\-]+[.][a-zA-Z]{2,4}/)(?:[^\\s()<>]+))\"\n",
    "    return re.sub(url, \"http\", data)\n",
    "\n",
    "#Lowercase all\n",
    "def lowercase_all(data):\n",
    "    return data.lower()\n",
    "\n",
    "def split_num(data):\n",
    "    result = []\n",
    "    current_num = \"\"\n",
    "\n",
    "    for char in data:\n",
    "        if char.isdigit():\n",
    "            current_num += char\n",
    "        else:\n",
    "            if current_num:\n",
    "                result.append(current_num)\n",
    "                current_num = \"\"\n",
    "            result.append(char)\n",
    "\n",
    "    if current_num:\n",
    "        result.append(current_num)\n",
    "\n",
    "    return \" \".join(result)\n",
    "\n",
    "def split_spec_char(data):\n",
    "    ch = \"@#!$%^&*-_(){}[]\\\\/:,.﻿\"\n",
    "    result = \"\"\n",
    "    for i in range(len(data)):\n",
    "        c = data[i]\n",
    "        if (c in ch):\n",
    "            if (i < len(data) - 1 and data[i + 1] in ch): \n",
    "                c = c + \" \"\n",
    "        result = result + c\n",
    "    return result\n",
    "    \n",
    "def process_data(data):\n",
    "    data = lowercase_all(data)\n",
    "    data = remove_url(data)\n",
    "    data = split_num(data)\n",
    "    data = split_spec_char(data)\n",
    "    data = truncate(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@ @   x i n   c h à o @   @'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_data('@@ Xin chào@ @')\n",
    "# process_data(train_vi[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_in = ['vietlao/train2023.lo', \n",
    "            'vietlao/train2023.vi', \n",
    "            'vietlao/dev2023.lo', \n",
    "            'vietlao/dev2023.vi', \n",
    "            'vietlao/test_lo.txt', \n",
    "            'vietlao/test_vi.txt']\n",
    "paths_out = ['train2023_clean.lo', \n",
    "            'train2023_clean.vi', \n",
    "            'dev2023_clean.lo', \n",
    "            'dev2023_clean.vi', \n",
    "            'test_lo_clean.txt', \n",
    "            'test_vi_clean.txt']\n",
    "\n",
    "for i, path in enumerate(paths_in):\n",
    "    datas = []\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            datas.append(line.strip())\n",
    "    f.close()\n",
    "\n",
    "    with open(paths_out[i], \"w\", encoding=\"utf-8\") as file:\n",
    "        for example in datas:\n",
    "            example = process_data(example)\n",
    "            file.write(example + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "\n",
    "# def remove_punctuation(example):\n",
    "#     translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "#     return example.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def remove_special_characters(example):\n",
    "#     chars = re.escape(string.punctuation)\n",
    "#     return re.sub(r'['+chars+']', '', example)\n",
    "#     # # Biểu thức chính quy để chỉ giữ lại các ký tự tiếng Lào\n",
    "#     # lao_char_pattern = re.compile('[ກ-໙\\s]+', re.UNICODE)\n",
    "    \n",
    "#     # # Áp dụng biểu thức chính quy để loại bỏ các ký tự không phải tiếng Lào\n",
    "#     # cleaned_str = ''.join(lao_char_pattern.findall(example))\n",
    "    \n",
    "#     return cleaned_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(example):\n",
    "#     # Lower case\n",
    "#     example = example.lower()\n",
    "#     # Remove punctuation\n",
    "#     example = remove_punctuation(example)\n",
    "#     # Remove numbers\n",
    "#     example = ''.join([i for i in example if not i.isdigit()])\n",
    "#     # TODO: fix Remove special characters\n",
    "#     example = remove_special_characters(example)\n",
    "#     # Remove whitespace\n",
    "#     example = example.strip()\n",
    "#     # involve remove_stopwords(example)\n",
    "#     example = tokenizeAndDetokenizeLo(example)\n",
    "\n",
    "#     # print(example)\n",
    "#     return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ຕ ົ ວຢ ່ າງ ໃນ ພາກ ທ ີ ພວກເຮ ົ າ ໄດ ້ ເຫ ັ ນ ວ ິ ທ ີ ການ sharad goel winter mason ແລະ duncan watts ສ ້ າງ ເກມ ທ ີ ່ ເອ ີ ້ ນ ວ ່ າ friendsense ເຊ ິ ່ ງ ຕ ົ ວຈ ິ ງ ແລ ້ ວ ແມ ່ ນ ຮ ູ ບພາບ ທ ີ ່ ສວຍງາມ ໃນ ການ ສ ໍ າ ຫ ຼ ວດ ທ ັ ດສະນະຄະຕ ິ ໃນ ບ ົ ດ ທ ີ ພວກເຮ ົ າ ໄດ ້ ເຫ ັ ນ ວ ິ ທ ີ ທ ີ ່ ທ ່ ານ ສາມາດ ສ ້ າງ ຂ ໍ ້ ມ ູ ນຄ ່ າໃຊ ້ ຈ ່ າຍ ທ ີ ່ ບ ໍ ່ ຖ ື ກຕ ້ ອງ ໂດຍ ການອອກແບບ ປະສ ົ ບການທ ີ ່ ປະຊາຊ ົ ນ ຕ ້ ອງການຢາກ ເຊ ັ ່ ນ ການທ ົ ດລອງ ດາວໂຫລດ ດ ົ ນຕ ີ ທ ີ ່ ຂ ້ ອຍ ສ ້ າງ ດ ້ ວຍ peter dodds ແລະ duncan watts salganik dodds and watts ສ ຸ ດທ ້ າຍ ໃນ ພາກ ທ ີ ພວກເຮ ົ າ ໄດ ້ ເຫ ັ ນ ວ ິ ທ ີ ການ kevin schawinski chris lintott ແລະ ທ ີ ມງານ galaxy zoo ສ ້ າງ ການຮ ່ ວມມ ື ທ ີ ່ ມ ີ ຂະຫນາດ ໃຫຍ ່ ທ ີ ່ ສ ົ ່ ງ ຜ ົ ນກະທ ົ ບ ຕ ໍ ່ ຫ ຼ າຍ ກວ ່ າ ຄ ົ ນ ທ ີ ່ ຈະ ເຂ ົ ້ າຮ ່ ວມ ໃນ ວຽກງານ labeling ຮ ູ ບພາບ ໃນ ສອງ ທ ິ ດທາງ ຂອງ ຄ ໍ າ ສ ັ ບ lintott et al ໃນ ແຕ ່ ລະ ກ ໍ ລະນ ີ ເຫ ຼ ົ ່ າ ນ ີ ້ ນ ັ ກ ຄ ົ ້ ນຄວ ້ າ ໄດ ້ ສ ຸ ມ ໃສ ່ ການສ ້ າງ ປະສ ົ ບການທ ີ ່ ດ ີ ສ ໍ າ ລ ັ ບ ຜ ູ ້ ເຂ ົ ້ າຮ ່ ວມ ແລະ ໃນ ແຕ ່ ລະ ກ ໍ ລະນ ີ ວ ິ ທ ີ ການ ນ ີ ້ ເຂ ົ ້ າໃຈ ເຖ ິ ງ ຈ ຸ ດປະສ ົ ງ ຂອງ ຜ ູ ້ ເຂ ົ ້ າຮ ່ ວມ ສາມາດ ເຮ ັ ດໃຫ ້ ການຄ ົ ້ ນຄວ ້ າ ໃຫມ ່'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = 'ຕົວຢ່າງ  ໃນ ພາກ ທີ   ພວກເຮົາ ໄດ້ ເຫັນ ວິທີການ sharad goel  winter mason  ແລະ duncan watts    ສ້າງ ເກມ ທີ່ເອີ້ນ ວ່າ friendsense  ເຊິ່ງ ຕົວຈິງ ແລ້ວ ແມ່ນ ຮູບພາບ ທີ່ ສວຍງາມ ໃນ ການ ສໍ າ ຫຼວດ ທັດສະນະຄະຕິ  ໃນ ບົດ ທີ   ພວກເຮົາ ໄດ້ ເຫັນ ວິທີ ທີ່ ທ່ານ ສາມາດ ສ້າງ ຂໍ້ມູນຄ່າໃຊ້ຈ່າຍ ທີ່ ບໍ່ ຖືກຕ້ອງ ໂດຍ ການອອກແບບ ປະສົບການທີ່ ປະຊາຊົນ ຕ້ອງການຢາກ  ເຊັ່ນ  ການທົດລອງ ດາວໂຫລດ ດົນຕີ ທີ່ ຂ້ອຍ ສ້າງ ດ້ວຍ peter dodds ແລະ duncan watts  salganik  dodds  and watts    ສຸດທ້າຍ  ໃນ ພາກ ທີ   ພວກເຮົາ ໄດ້ ເຫັນ ວິທີການ kevin schawinski  chris lintott ແລະ ທີມງານ galaxy zoo ສ້າງ ການຮ່ວມມື ທີ່ມີ ຂະຫນາດ ໃຫຍ່ ທີ່ ສົ່ງ ຜົນກະທົບ ຕໍ່ ຫຼາຍ ກວ່າ  ຄົນ ທີ່ຈະ ເຂົ້າຮ່ວມ ໃນ ວຽກງານ labeling ຮູບພາບ  ໃນ ສອງ ທິດທາງ ຂອງ ຄໍ າ ສັບ   lintott et al    ໃນ ແຕ່ລະ ກໍລະນີ ເຫຼົ່າ ນີ້  ນັກ ຄົ້ນຄວ້າ ໄດ້ ສຸມ ໃສ່ ການສ້າງ ປະສົບການທີ່ ດີ ສໍ າ ລັບ ຜູ້ ເຂົ້າຮ່ວມ  ແລະ ໃນ ແຕ່ລະ ກໍລະນີ  ວິທີການ ນີ້ ເຂົ້າໃຈ ເຖິງ ຈຸດປະສົງ ຂອງ ຜູ້ ເຂົ້າຮ່ວມ ສາມາດ ເຮັດໃຫ້ການຄົ້ນຄວ້າ ໃຫມ່'\n",
    "# preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def remove_special_characters_vi(example):\n",
    "#     return re.sub(r'[^a-zA-ZĐđÀ-ỹà-ỹĂăÂâÊêÔôƠơƯư\\s]+', '', example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_vi(example):\n",
    "#     # Lower case\n",
    "#     example = example.lower()\n",
    "#     # Remove punctuation\n",
    "#     example = remove_punctuation(example)\n",
    "#     # Remove numbers\n",
    "#     example = ''.join([i for i in example if not i.isdigit()])\n",
    "#     # involve Remove special characters\n",
    "#     example = remove_special_characters(example)\n",
    "#     # Remove whitespace\n",
    "#     example = example.strip()\n",
    "#     # \n",
    "#     # involve Remove stopwords\n",
    "#     example = tokenizeAndDetokenizeVi(example)\n",
    "\n",
    "#     return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lo = []\n",
    "path_train = 'train5.lo'\n",
    "\n",
    "with open(path_train, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        train_lo.append(line.strip())\n",
    "f.close()\n",
    "\n",
    "train_vi = []\n",
    "path_train = 'train5.vi'\n",
    "\n",
    "with open(path_train, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        train_vi.append(line.strip())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "679\n",
      "40.18795\n"
     ]
    }
   ],
   "source": [
    "train_input_max_len = -1 # độ dài câu lớn nhất có trong bộ train\n",
    "sum_len = 0\n",
    "for sent in train_lo:\n",
    "  sent_len = len(sent.split(\" \"))\n",
    "  train_input_max_len = max(train_input_max_len, sent_len)\n",
    "  sum_len += sent_len\n",
    "train_input_len_list = [0] * (train_input_max_len + 1)\n",
    "for sent in train_lo:\n",
    "  sent_len = len(sent.split(\" \"))\n",
    "  sent_len = max(0, sent_len)\n",
    "  train_input_len_list[sent_len] += 1\n",
    "print(train_input_max_len)\n",
    "print(sum_len/len(train_lo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "24.054897739504845\n"
     ]
    }
   ],
   "source": [
    "train_label_max_len = -1 # độ dài câu lớn nhất có trong bộ test\n",
    "sum_len = 0\n",
    "for sent in train_vi:\n",
    "  sent_len = len(sent.split(\" \"))\n",
    "  train_label_max_len = max(train_label_max_len, sent_len)\n",
    "  sum_len += sent_len\n",
    "train_label_len_list = [0] * (train_label_max_len + 1)\n",
    "for sent in train_vi:\n",
    "  sent_len = len(sent.split(\" \"))\n",
    "  sent_len = max(0, sent_len)\n",
    "  train_label_len_list[sent_len] += 1\n",
    "print(train_label_max_len)\n",
    "print(sum_len/len(train_vi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_no_of_length = [num_sent_len_i for num_sent_len_i in train_input_len_list]    # số câu có độ dài = index\n",
    "train_length = [i for i in range(train_input_max_len+1)]                            # 0 -> độ dài max có thể của câu trong tập train\n",
    "train_y_no_of_length = [i for i in train_label_len_list]\n",
    "test_length = [i for i in range(train_label_max_len+1)]\n",
    "\n",
    "fig, pl = plt.subplots(1, 2, figsize = (15, 5))\n",
    "\n",
    "pl[0].bar(train_length, train_x_no_of_length, color = \"dodgerblue\")\n",
    "pl[0].set_xlabel(\"sentence length\")\n",
    "pl[0].set_ylabel(\"number of sentences\")\n",
    "pl[0].set_title(\"lao (bible_raw_data)\")\n",
    "\n",
    "pl[1].bar(test_length, train_y_no_of_length, color = \"dodgerblue\")\n",
    "pl[1].set_xlabel(\"sentence length\")\n",
    "pl[1].set_ylabel(\"number of sentences\")\n",
    "pl[1].set_title(\"vietnamese (bible_raw_data)\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
